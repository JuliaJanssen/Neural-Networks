{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author contributions\n",
    "Please fill out for each of the following parts who contributed to what:\n",
    "- Conceived ideas: \n",
    "- Performed math exercises: \n",
    "- Performed programming exercises:\n",
    "- Contributed to the overall final assignment: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "## Multilayer perceptron\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Brightspace before the deadline (see Brightspace).\n",
    "    \n",
    "Learning goals:\n",
    "1. Understand and implement a multi-layer perceptron (MLP) with two weight layers\n",
    "1. Derive and implement backpropagation\n",
    "1. Get familiar with the role of softmax units in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the architecture\n",
    "\n",
    "You are about to implement a multi-layer perceptron (MLP), using backpropagation to learn weights for classifying the 10 MNIST handwritten digits. The input, hidden, and output node layers are connected with two weight layers. The $n_h$ hidden layer nodes use sigmoid activations, and the 10 output layer nodes use softmax activations; learning the one-hot encoding / representation of the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Properties of activation functions (0.5 points)\n",
    "\n",
    "The activation functions we have discussed so far were nonlinear. This property is actually required for MLPs to work. Show with a simple mathematical proof that with a linear activation function $f(a) = b a$ (where b is some constant, e.g. $b=2.3$) the forward pass of a multilayer perceptron (MLP) with two weight layers can be written as the forward pass of a single layer perceptron (SLP), and hence could be done more efficiently by a neural network with a single weight layer. \n",
    "\n",
    "Hint: The forward pass of a neural network with the weight layers $W^1$ and $W^2$ is $y = g( W^2 h(W^1 x) )$. Here, both $g$ and $h$ are linear activation functions like $f$, but they can have any real valued scalar $b$ (e.g., $g(a)=1.8a$ and $h(a)=1.1a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1:\n",
    "$h(x) = b_1 a_1$ \n",
    "\n",
    "$h(x) = b_1 W^1 x$\n",
    "\n",
    "\n",
    "$g(x) = b_2 a_2$ \n",
    "\n",
    "$g(x) = b_2 W^2 h(a_1)$\n",
    "\n",
    "$g(x) = b_2 W^2 b_1 W^1 x$\n",
    "\n",
    "$y = b_2 W^2 b_1 W^1 x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Properties of weight initialization (0.5 points)\n",
    "\n",
    "Initialization of your network's parameters (e.g. weights) can have significant impact on the performance of your network. Consider the scenario where we initialize all parameters with the same constant value. Why would this prevent your network from learning anything sensible and be detrimental to your network's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "If all the weights are initialized with the same constant, the derivative with respect to the loss function is then the same for all the weights and this means that the weights stay symmetric is subsequent iterations and the model is essentially restricted to linear problems and this prevents the network from learning anything sensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The sigmoid activation function (0.5 points)\n",
    "\n",
    "The hidden layer units apply the sigmoid function on their linear activations $a$: \n",
    "\n",
    "$$f(a) = \\frac{1}{1+\\exp(-a)}$$\n",
    "\n",
    "To compute backpropagation you will need its derivative again, and you have learned that the sigmoid function derivative has a very simple form. \n",
    "\n",
    "Express $\\frac{\\partial f(a)}{\\partial a} = \\frac{\\partial h}{\\partial a}$ in terms of this simple form, and expressed only by the hidden unit output $h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Solution 3\n",
    "$ \\frac{\\partial f(a)}{\\partial a} = f(a)(1 - f(a)) $\n",
    "\n",
    "$ \\frac{\\partial h}{\\partial a} = h(1 - h) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus exercise: The softmax cross entropy loss (1 bonus point)\n",
    "\n",
    "Our MLP should do multi-class classification, i.e. be able to classify all 10 digits in MNIST, not just two. A single output unit with a sigmoid unit would be a 2-way output and would not work here. Instead we use as many output units as we have classes - for MNIST classification, the output is then a binary vector of length 10. Each output represents the probability for the associated class given a certain input. \n",
    "\n",
    "This means that in the training set the output unit (vector element) corresponding to the right class has the value 1 (probability 1), and all others are 0. This is a so-called *one-hot encoding* of class labels. Here, a good activation function is the *softmax* activation function, defined as: \n",
    "\n",
    "$$ y_k = p(z_k) = \\frac{\\exp(z_k)}{\\sum_{l=1}^K\\exp(z_l)} $$\n",
    "\n",
    "where $K$ represents the number of output units (= classes), and $z_k$ is the activation going into a single of these output units. With softmax, if you want to classify 10 digits, you define 10 output units and apply softmax over the output of each of them. Then the resulting 10 values will: \n",
    "* sum up to 1. \n",
    "* all be in the range $[0,1]$. \n",
    "\n",
    "These properties make it useful for getting the desired probability distribution as output. The output class predicted in the forward pass could then just be the one with the highest probability. \n",
    "\n",
    "For learning the right weights we again combine this activation function with the *cross-entropy cost function*: \n",
    "\n",
    "$$ L = - \\sum_{l=1}^K t_l \\lg (y_l) $$\n",
    "\n",
    "Note that if we would have 2 classes like in the previous assignment, we would have the same definition $L = -t \\log(y) - (1 - t)\\log(1 - y)$ again, as $t_2 = 1 - t_1$ and $y_2 = 1 - y_1$.\n",
    "\n",
    "\n",
    "Taking the derivative of the cross entropy loss $\\frac{\\partial L}{\\partial z_k}$ for the softmax and a single softmax input / activation $z_k$, we will get: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_k} = \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial z_k} = y_k - t_k$$\n",
    "\n",
    "\n",
    "**Bonus assignments (0.5 points each)**: \n",
    "1. Show how to derive the softmax activation function. You will need to show this for the two cases $i=j: \\frac{\\partial y_i}{\\partial z_i}$ and $i \\neq j: \\frac{\\partial y_i}{\\partial z_j}$. \n",
    "1. Use your result to derive the cross entropy loss for the softmax function $\\frac{\\partial L}{\\partial z_k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "$\\frac{\\delta y_i}{\\delta z_i} = \\frac{\\delta \\frac{exp(z_i)}{\\sum_{l=1}^K exp(z_l)}}{\\delta z_i}$\n",
    "\n",
    "Taking the derivative:\n",
    "\n",
    "If i = j:\n",
    "\n",
    "$ \\frac{\\delta y_i}{\\delta z_i} = \\frac{exp(z_i) \\sum_{l=1}^K exp(z_l) - exp(z_i) exp(z_i)}{(\\sum_{l=1}^K exp(z_l))^2} $\n",
    "\n",
    "$ \\frac{\\delta y_i}{\\delta z_i} = \\frac{exp(z_i) (\\sum_{l=1}^K exp(z_l) - exp(z_i))}{(\\sum_{l=1}^K exp(z_l))^2} $\n",
    "\n",
    "$ \\frac{\\delta y_i}{\\delta z_i} = \\frac{exp(z_i)}{\\sum_{l=1}^K exp(z_l)} \\cdot \\frac{\\sum_{l=1}^K exp(z_l) - exp(z_i)}{\\sum_{l=1}^K exp(z_l)} $\n",
    "\n",
    "$ \\frac{\\delta y_i}{\\delta z_i} = p(z_i) (1 - p(z_l)) $\n",
    "\n",
    "\n",
    "If i $\\neq$ j:\n",
    "\n",
    "$ \\frac{\\delta y_i}{\\delta z_j} = \\frac{0 - exp(z_j) exp(z_i)}{(\\sum_{l=1}^K exp(z_l))^2} $\n",
    "\n",
    "$ \\frac{\\delta y_i}{\\delta z_j} = \\frac{- exp(z_j)}{\\sum_{l=1}^K exp(z_l)} \\cdot \\frac{exp(z_i)}{\\sum_{l=1}^K exp(z_l)} $\n",
    "\n",
    "$ \\frac{\\delta y_i}{\\delta z_j} = -p(z_j) \\cdot p(z_i)$\n",
    "\n",
    "2.\n",
    "\n",
    "$ L = - \\sum_{l=1}^K t_l log(y_l) $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = - \\sum_{i=1}^K t_i \\frac{\\delta log(y_i)}{\\delta z_k} $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = - \\sum_{i=1}^K t_i \\frac{\\delta log(y_i)}{\\delta y_i} \\cdot \\frac{\\delta y_i}{\\delta z_k} $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = - \\sum_{i=1}^K t_i \\frac{1}{y_i} \\cdot \\frac{\\delta y_i}{\\delta z_k} $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = -t_k (1-y_k) - \\sum_{i \\neq k}^K t_i \\frac{1}{y_i} \\cdot (-y_i \\cdot y_k) $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = -t_k (1-y_k) + \\sum_{i \\neq 1}^K t_i y_k $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = -t_k + t_k y_k + \\sum_{i \\neq 1}^K t_i y_k $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = y_k (t_k + \\sum_{i \\neq 1}^K t_i) - t_k $\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta z_k} = y_k - t_k $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: The forward pass (1 point)\n",
    "\n",
    "The inputs of the network $x$ are MNIST images. In the forward pass a single training data example $x$ (a vector of size $m \\times 1$) is weighted by a first weight layer $W^1$ (size $n_h \\times m$). Then this activation $a$ is passed into the sigmoid activation function, producing the hidden layer activation $h$ (a vector of size $n_h \\times 1$). The hidden unit values $h$ are then weighted by a second layer of weights $W^2$ (size $10 \\times n_h$), producing the output unit activation $z$ (a vector of size $10 \\times 1$). Then, for each class $k$ there is an output unit with a softmax activation. \n",
    "\n",
    "Write down the equations for the activations $a$, $h$, $z$ and a single output unit $y_k$.\n",
    "\n",
    "Note that in the implementation you can easily compute all $y_k$ at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "a &=&   xW^1 \\\\\n",
    "h &=& \\frac{1}{1+\\exp(-a)}\\\\ \n",
    "z &=& hW^2\\\\\n",
    "y_k &=&  \\frac{\\exp(z_k)}{\\sum_{l=1}^K\\exp(z_l)}\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Gradient of the last layer (1 point)\n",
    "\n",
    "To update the weights so to gradually let the network learn the classification, we need the partial derivatives of the weights. To compute the partial derivatives of the weights $W^2$ in the last layer, we have to propagate from the error function back through the softmax activation function to the weights. Obtain $\\frac{\\partial L}{\\partial W^2_{ij}}$ (i.e., $i$ is output, $j$ is input) by applying the chain rule multiple times. $L$ is the cross-entropy loss, and it receives the result of the $K$ softmax output units. \n",
    "\n",
    "Start with writing down the chain of partial derivatives, and then fill in the bits and pieces, to arrive at a trivial equation for this gradient.\n",
    "\n",
    "Notes:\n",
    "1. Here we derive the gradients for individual data examples. In the examples you will work with multiple examples at once. Basically, we sum all these gradients in order to do one big weight update. Think carefully how you can do this efficiently in your implementation.\n",
    "1. In the implementation you should obtain all weight updates (i.e., all $i$'s and $j$'s) at once with a single matrix multiplication. Think carefully whether this is a matrix multiplication (np.dot), or an element wise multiplication (*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^2_{ij}} = \\frac{\\partial L}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial z_{i}} \\frac{\\partial z_{i}}{\\partial W_{ij}^2} $\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^2_{ij}} = \\frac{\\partial L}{\\partial z_{i}} \\frac{\\partial z_{i}}{\\partial W_{ij}^2} $\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^2_{ij}} = (y_{i} - t_{i}) \\cdot h_{j} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Gradient of the first layer (1.5 points)\n",
    "\n",
    "To compute the partial derivatives of the weights $W^1$ in the first layer, we have to propagate from the error function back through the last layer into the first layer (i.e., apply backproagation).  In a similar vein as above, using a sequence of the chain rule, derive the chain of partial derivatives to compute $\\frac{\\partial L}{\\partial W^1_{ij}}$ (i.e., $i$ is output, $j$ is input).\n",
    "\n",
    "Start with writing down the chain of partial derivatives, and then fill in the bits and pieces, to arrive at a trivial equation for this gradient.\n",
    "\n",
    "Notes:\n",
    "1. Here we derive the gradients for individual data examples. In the examples you will work with multiple examples at once. Basically, we sum all these gradients in order to do one big weight update. Think carefully how you can do this efficiently in your implementation.\n",
    "1. In the implementation you should obtain all weight updates (i.e., all $i$'s and $j$'s) at once with a single matrix multiplication. Think carefully whether this is a matrix multiplication (np.dot), or an element wise multiplication (*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6:\n",
    "$\\frac{\\partial L}{\\partial W^1_{ij}} = \\frac{\\partial L}{\\partial a_{i}} \\frac{\\partial a_{i}}{\\partial W^1_{ij}}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^1_{ij}} = \\sum_{k} \\frac{\\partial L} {\\partial z_k} \\frac{\\partial z_k} {\\partial a_i} x_{j}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^1_{ij}} = \\sum_{k} (y_{k} - t_{k}) \\cdot \\frac{\\partial z_k}{\\partial a_i} \\cdot x_{j} $\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^1_{ij}} = \\sum_{k} (y_{k} - t_{k}) \\cdot W^2_k \\cdot \\frac{\\partial h}{\\partial a} \\cdot x_{j} $\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^1_{ij}} = \\sum_{k} (y_{k} - t_{k}) \\cdot W^2_k \\cdot h(1 - h) \\cdot x_{j} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Implementation (3 points)\n",
    "Now that all the math is done, we can start implementing the two-layer network for binary classification of two digits, where we make use of sigmoid units and the cross-entropy loss. Write the following functions:\n",
    "1. `cross_entropy(Y, T)`: Computes the cross entropy loss, averaged over examples `N`. Make sure that there are no exact-zero inputs for `np.log()` (something simple like adding a very small number to `Y` is fine). Note that you sum $L$ over the classes `K` (check the `axis=` parameter of `np.sum`).\n",
    "1. `sigmoid(A)`: Passes the activity matrix `A` through the sigmoid activation function.\n",
    "1. `softmax(A)`: Passes the activity matrix `A` through the softmax activation function. Note that you can compute softmax in one literal line. You just need to sum the denominator over the right dimension. \n",
    "1. `linear(X, W)`: Computes the activities `A` as `X` weighted by `W`.\n",
    "1. `forward(X, W1, W2)`: Computes the forward pass for the two-layer network with sigmoid activations in the first and softmax activations in the second node layer. Returns `Y` and `H`. \n",
    "1. `backward(X, H, Y, W2, T)`: Computes the backward pass for the two-layer network with sigmoid units and cross-entropy loss.\n",
    "1. `train_network(X_train, T_train, X_val, T_val, n_epochs, eta)`: Implements the training procedure (train the model on training data, and evaluate on both training and validation data). See the skeleton code for some help. Use the fuction `initialize_weights(n_in, n_out)` to initialize your weights with the right shapes.\n",
    "1. `test_network(X, W)`: Predicts class labels for a set of `N` new and unseen training data examples, given as `X`. Interpret the output vector of length `10` as a set of probabilities for the class labels `0, 1, 2, 3, 4, 5, 6, 7, 8, 9` (Integer values). For each of the `N` output vectors, return the class label with the highest probability. \n",
    "\n",
    "Last time we initialized the weights from a Gaussian normal distribution. This time we initialize them by drawing uniformly from the rule of thumb range $ \\left [ - \\frac{ \\sqrt{6} }{ \\sqrt{n + m} },  \\frac{ \\sqrt{6} }{ \\sqrt{ n + m} } \\right ] $ ($n\\times m$ being the weight matrix dimensions), which  works better here. Weight initialization can have quite some influence on your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(P, Q):\n",
    "    \"\"\"\n",
    "    Initializes a weight matrix.\n",
    "    INPUT:\n",
    "        P = [int] number of input units\n",
    "        Q = [int] number of output units\n",
    "    OUTPUTS\n",
    "        W = [Q P] the initial weight matrix of Q outputs by P inputs\n",
    "    \"\"\"\n",
    "    r = np.sqrt(6) / np.sqrt(Q + P)\n",
    "    return np.random.uniform(-r, r, [Q, P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y, T):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss.\n",
    "    INPUT:\n",
    "        Y = [K N] output vector for N examples and K units (classes)\n",
    "        T = [K N] target vector for N examples and K units (classes)\n",
    "    OUTPUTS\n",
    "        L = [flt]  the mean cross-entropy loss\n",
    "    \"\"\"\n",
    "    Y[Y==0] += 0.00000000000001\n",
    "    Y[Y==1] -= 0.00000000000001\n",
    "    L = -T * np.log(Y) - (1 - T) * np.log(1 - Y)\n",
    "    L = np.mean(L)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes the softmax activation function. \n",
    "    INPUT:\n",
    "        Z = [10 N] vector of input activations for 10 output units and N examples\n",
    "    OUTPUTS\n",
    "        Y = [10 N] the vectors of softmax activations for 10 output units and N examples\n",
    "    \"\"\"\n",
    "    Z -= np.max(Z, axis=0)\n",
    "    Z = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(A):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "    INPUT:\n",
    "        A = [H N] activity matrix of H units for N examples\n",
    "    OUTPUT\n",
    "        Y = [H N] output matrix of H units for N examples\n",
    "    \"\"\"\n",
    "    Y = 1 / (1 + np.exp(-A))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X, W):\n",
    "    \"\"\"\n",
    "    Computes the activities for a fully connected layer.\n",
    "    INPUT:\n",
    "        X = [P N] data matrix of P input units for N examples\n",
    "        W = [Q P] weight matrix of P inputs to Q outputs\n",
    "    OUTPUT\n",
    "        A = [Q N] activity matrix of Q output units for N examples\n",
    "    \"\"\"\n",
    "    A = np.dot(W, X)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, W2):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a two-layer network with sigmoid units.\n",
    "    INPUT\n",
    "        X  = [P  N] data matrix of P inputs for N examples\n",
    "        W1 = [Q  P] weight matrix of the first layer of P inputs to Q outputs\n",
    "        W2 = [10 Q] weight vector of the second layer of Q inputs to 10 outputs\n",
    "    OUTPUT\n",
    "        H = [Q  N] output matrix of Q hidden units for N examples\n",
    "        Y = [10 N] output vector for N examples\n",
    "    \"\"\"\n",
    "    H = sigmoid(linear(X, W1))\n",
    "    Y = softmax(linear(H, W2))\n",
    "    return H, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, H, Y, W2, T):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a two-layer network with sigmoid and softmax units, and cross-entropy loss.  \n",
    "    INPUT:\n",
    "        X  = [P  N] data matrix of P inputs for N examples\n",
    "        H  = [Q  N] output matrix of Q hidden units for N examples\n",
    "        Y  = [10 N] output probability vectors for N examples\n",
    "        W2 = [10 Q] weight vector of the second layer of Q inputs to 10 outputs\n",
    "        T  = [10 N] a vector of one-hot encoded targets for N examples\n",
    "    OUTPUT\n",
    "        dW1 = [Q  P] gradient matrix for the weights of layer 1 of P inputs to Q outputs\n",
    "        dW2 = [10 Q] gradient matrix for the weights of layer 2 of Q inputs to 10 outputs\n",
    "    \"\"\"\n",
    "    dW2 = np.dot((Y - T), H.T)\n",
    "    sigmoid_derivative = H * (1 - H)\n",
    "    error1 = np.sum(np.dot(W2.T, (Y - T))) * sigmoid_derivative\n",
    "    dW1 = np.dot(error1, X.T)\n",
    "    return dW1, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X_train, T_train, X_val, T_val, n_hidden=30, n_epochs=500, eta=10**-3):\n",
    "    \"\"\"\n",
    "    Performs the training procedure for a two-layer network with sigmoid and softmax units, and cross-entropy loss. \n",
    "    INPUT:\n",
    "        X_train  = [P  N] data matrix of P inputs for N training examples\n",
    "        T_train  = [10 N] a vector of targets for N training examples\n",
    "        X_val    = [P  M] data matrix of P inputs for M validation examples\n",
    "        T_val    = [10 M] a vector of targets for M validation examples\n",
    "        n_hidden = [int]  number of hidden units (default 30)\n",
    "        n_epochs = [int]  number of training epochs (default 500)\n",
    "        eta      = [flt]  learning rate (default 10^-3)\n",
    "    OUTPUT:\n",
    "        W1         = [Q  P] the learned weights for layer 1 of P inputs to Q outputs\n",
    "        W2         = [10 Q] the learned weights for layer 2 of Q inputs to 10 outputs\n",
    "        train_loss = [Z  1] the training loss for Z epochs\n",
    "        val_loss   = [Z  1] the validation loss for Z epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize W1 and W2 (use initialize_weights())\n",
    "    W1 = initialize_weights(len(X_train), n_hidden)\n",
    "    W2 = initialize_weights(n_hidden, 10)\n",
    "    \n",
    "    # Loop over epochs\n",
    "    train_loss = np.zeros((n_epochs))\n",
    "    val_loss = np.zeros((n_epochs))\n",
    "    for i_epoch in range(n_epochs):\n",
    "        \n",
    "        # Forward pass\n",
    "        H_train, Y_train = forward(X_train, W1, W2)\n",
    "        H_val, Y_val = forward(X_val, W1, W2)\n",
    "        \n",
    "        # Backward pass\n",
    "        dW1, dW2 = backward(X_train, H_train, Y_train, W2, T_train)\n",
    "\n",
    "        # Parameter update\n",
    "        W1 = W1 - eta * dW1\n",
    "        W2 = W2 - eta * dW2\n",
    "        \n",
    "        # Save loss\n",
    "        train_loss[i_epoch] = cross_entropy(Y_train, T_train)\n",
    "        val_loss[i_epoch] = cross_entropy(Y_val, T_val)\n",
    "        \n",
    "        # Print progress and loss\n",
    "        if i_epoch % 100 == 0:\n",
    "            print(\"Epoch {}/{}. Train loss: {:.3f}. Validation loss: {:.3f}.\".format(\n",
    "                1+i_epoch, n_epochs, train_loss[i_epoch], val_loss[i_epoch]))\n",
    "        \n",
    "    return W1, W2, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(X, W1, W2):\n",
    "    \"\"\"\n",
    "    Applies the trained two-layer network with sigmoid units to data.\n",
    "    INPUT:\n",
    "        X  = [P  N] data matrix of P inputs for N examples\n",
    "        W1 = [Q  P] weight matrix of the first layer of P inputs to Q outputs\n",
    "        W2 = [10 Q] weight vector of the second layer of Q inputs to 10 outputs\n",
    "    OUTPUT\n",
    "        classes = [1 N] predicted integer labels from 0 to 9 for N examples\n",
    "    \"\"\"\n",
    "    H = np.dot(W1, X)\n",
    "    classes = np.dot(W2, H)\n",
    "    classes = classes.round()\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "Below we first load in (a subset of) the MNIST handwritten digit dataset, and restrict it to two digits. We plot some examples. We split this data into a training and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read full dataset from mat file\n",
    "mat = sio.loadmat(\"digits.mat\")\n",
    "\n",
    "# The data set contains 1000 examples of each class in sequence - create the corresponding label vector: \n",
    "T = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).repeat(1000)\n",
    "\n",
    "# The 1000 examples of the 0-class are currently at the end of the data set. \n",
    "# Move them to the beginning. Then we can use np.argmax to get from one-hot encoded class probabilites to\n",
    "# the original class label: \n",
    "X = np.roll(mat[\"digits\"], shift=1000, axis=1)\n",
    "\n",
    "sz = (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAABVCAYAAACsCb4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAFu0lEQVR4nO3d0Y7jNgwAwE3R///l9KEIstcmtkzJEiXNPB3Qy6GhaTkyafrxfD5/AAAAuOav0f8DAAAAM7KZAgAACLCZAgAACLCZAgAACLCZAgAACLCZAgAACPj76D8+Hg9z0794Pp+PyOfE9DsxbS8a058fcT0iV9sT0/bEtD0xbU9M2xPT9o5iqjIFAAAQYDMFAAAQYDMFAAAQYDMFAAAQcDiAAgAgq+fz/8/LPx7huTsAl6lMAQAABNhMAQAABGjzAwDS+9TSBzCayhQAAEDA9JWpoztVHkIFgHldqUa55gMjqEwBAAAE2EwBAAAETNnmV1r2//33lP/b8E4PRmj14Llcbc86W+5s/dS2/nZ2zu8WD9bmt9XcVKYAAAACpqlMGYk6lviXqY3TbneieueVKgq1WuestRVgbipTAAAAATZTAAAAAenb/CItEK/2nd+fff1Zaw+taM+JE7tyWdeuVY9hr++V7XhmYOhEDqXnwK7HY9W1L4ua/BvVyq8yBQAAEJCqMtVjt+8B9HLuEr7dMVhi57tbn6rHvIkLOzESvq9W64vfU+3tGsdITp59pmdXh8oUAABAgM0UAABAQKo2v1KRN8Zrmymn5aINsTp3R4yc6/f5FNvV8vzTdSNyzflEbr5pI29HXvXRajCH4/V253pqAAUAAEBywytTtTt9d6/acKfks6txkY+wjtLzufXfgzM11+wrebj7b4NW1Y9MVZTRVoyFyhQAAECAzRQAAEDAsDa/DG/Y7jmDfna7xmjX7z2bDOsJ+3ItKXfngKMdBqT0aD23nr6dDZ85O/d3H+h1Ry5ljKnKFAAAQEDXytSKD53NzPHob/eHeVtYOYZZ76yvHPOrXsdDTHI4Ow67Vg13+76Z+G1VbpVYqEwBAAAE2EwBAAAEDH/P1ColPqBO67Ypa0sdrSrHPj2Y/i1mu8eqlZo14vdnZzse2QZCzBa/HrT9/qnFkIgrMR2dkypTAAAAAV0qU3bsubjj3EarvN4x3taEvsT7XmdDKXYdgtBC7d3pFXL/bDx3q7zKOHI6k5rhM+J3HIPZY6oyBQAAEGAzBQAAEDBsAEWm8twOVmh1GEn8GKVH7p2tx/K/zFk7Vtb3iN0t8r1rhi6snK+t80Xbf7nZW9FGW/m8VJkCAAAIsJkCAAAIGP6eqTspX+d7P8VsatpySmM/8/tPonpMn5pRi5alXXIou9JjueP5/03tNds0ujJ+G5Vb7RrTy9XJh7O366pMAQAABCxZmXJ3iogWd0Fq/41dH1CPqnnvxyxGHf+VY0ofLXIoUo2yZn7mnKa3O87FjOe3yhQAAECAzRQAAEDAMm1+Hqh8M3SiXKu8ufPf0bZKFvKtnJaqNq7EUX7WEb+3q+2jzve9qUwBAAAETF+Zctf+OnE5NnL8bul4UMfwX+4GkoE8/KzHgBhrIa1ErrWuz/z8qEwBAACE2EwBAAAEDGvze5VGa1qqvtm1xKrVpNxRrLLl3Iz5fGfrgzwng9o8nPG8bsl5fC+PQJSTi9RSmQIAAAgYPoCi1R0Bd1roRa5917riF+H40JoqVJ3f3/9qLHePHfmoZPFfKlMAAAABNlMAAAABw9v8IpT9rxOzPx29z0ms5uOY9fG7vWW1mLdo3VktJncQo/uc5bDYXyemlFCZAgAACOhSmTqqAkQ+C3eQa/VeMbzzAV3HiVqt81NOMpIx6HV6XLdYm8oUAABAgM0UAABAwLABFErPsK5Ia681IZeadwNl5J2GwJGzNc+539e345FxWJjKFAAAQMCUo9H5LNMuHf5Lfs5rhWO3wneAF5X+e4lffple1aEyBQAAEGAzBQAAEKDNDwBgIp8ezh/d6gR3yT4QSWUKAAAgQGUKAGBSKlLsJGO+q0wBAAAE2EwBAAAEPDI+yAUAAJCdyhQAAECAzRQAAECAzRQAAECAzRQAAECAzRQAAECAzRQAAEDAPwYTleXQn08mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x504 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some examples\n",
    "fig, ax = plt.subplots(1, 10)\n",
    "fig.set_size_inches([15, 7])\n",
    "for i in range(10):\n",
    "    ax[i].imshow(X[:, 0 + i*1000].reshape(sz).T, cmap=\"gray\")\n",
    "    ax[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions training inputs: (784, 6400), and training outputs: (10, 6400)\n",
      "Dimensions validation inputs: (784, 1600), and validation outputs: (10, 1600)\n",
      "Dimensions testing inputs: (784, 2000), and testing outputs: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in training, validation, and testing split\n",
    "X_train, X_test, T_train, T_test = train_test_split(X.T, T, test_size=0.2)\n",
    "X_train, X_val, T_train, T_val = train_test_split(X_train, T_train, test_size=0.2)\n",
    "\n",
    "# Transpose back\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# Transform the label sets used in training to one-hot vectors: \n",
    "T_train = np.eye(10)[T_train].T\n",
    "T_val = np.eye(10)[T_val].T\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions training inputs: {}, and training outputs: {}\".format(X_train.shape, T_train.shape))\n",
    "print(\"Dimensions validation inputs: {}, and validation outputs: {}\".format(X_val.shape, T_val.shape))\n",
    "print(\"Dimensions testing inputs: {}, and testing outputs: {}\".format(X_test.shape, T_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Training (1 point)\n",
    "Now that we have done all the work, we can finally run the multilayer perceptron to learn classifying digits. \n",
    "\n",
    "1. Train your network on the training dataset `X_train` and `T_train`, and validate it at each epoch on the test set `X_val` and `T_val`. Use de default values for the number of hidden units, learning rate, and number of epochs.\n",
    "1. After training, plot the train and validation losses over epochs (as returned by `train_network()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500. Train loss: 1.112. Validation loss: 0.973.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stijn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500. Train loss: 1.107. Validation loss: 0.968.\n",
      "Epoch 201/500. Train loss: 1.107. Validation loss: 0.968.\n",
      "Epoch 301/500. Train loss: 1.107. Validation loss: 0.968.\n",
      "Epoch 401/500. Train loss: 1.107. Validation loss: 0.968.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23711a4cd08>]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train network\n",
    "n_hidden = 30\n",
    "n_epochs = 500\n",
    "eta = 10**-3\n",
    "W1, W2, train_loss, val_loss = train_network(X_train, T_train, X_val, T_val, n_hidden, n_epochs, eta)\n",
    "\n",
    "# Plot losses\n",
    "plt.plot(np.arange(0, n_epochs), train_loss)\n",
    "plt.plot(np.arange(0, n_epochs), val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Testing (1 point)\n",
    "Now that the network is trained, we can obtain a test score on a held out test set, and compute a classification performance. Apply your network to the test set `X_test` and `T_test`, and print its accuracy. \n",
    "\n",
    "If everything went fine, the accuracy should be above 90%, which is fine as we only use 1/6 of the original MNIST data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -780988.  -638914.  -841353. ...  -978592.  -790905.  -488945.]\n",
      " [ -845807.  -691053.  -910709. ... -1059594.  -856807.  -529431.]\n",
      " [ -775395.  -633864.  -834572. ...  -970843.  -783470.  -484919.]\n",
      " ...\n",
      " [ -758211.  -620129.  -816312. ...  -949213.  -766438.  -474294.]\n",
      " [ -703143.  -575940.  -757946. ...  -881197.  -712666.  -440421.]\n",
      " [ -819728.  -670552.  -882825. ... -1027120.  -830227.  -513059.]]\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test network\n",
    "classes = test_network(X_test, W1, W2)\n",
    "print(classes)\n",
    "# Print accuracy\n",
    "accuracy = 0\n",
    "correct = 0\n",
    "\n",
    "correct = np.count_nonzero(np.equal(classes, T_test))\n",
    "\n",
    "accuracy = correct / len(classes[0])\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has the network learned?\n",
    "\n",
    "What has the MLP learned to be able to classify the digits with relatively high accuracy? We can easily inspect a part of the network, that is the weights coming directly after the input nodes (i.e. the first layer weights) to check which patterns the MLP deemed important for correct classification at this stage (for higher layers this inspection is more complicated, and an active research field). \n",
    "\n",
    "For this we just need to reshape the first layer weights leading from all input values to a hidden unit to $28 \\times 28$. We do this separately for each hidden unit to check what each of them represents or detects. These patterns act a bit like *receptive fields*.\n",
    "\n",
    "When you have trained the full network, just run the code in the next cell to check the learned pattern detectors in the first layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_hidden = W1.shape[0]\n",
    "\n",
    "plt.figure(figsize=([8,10]))\n",
    "for i in range(n_hidden):    \n",
    "    plt.subplot(5,6,i+1)\n",
    "    fig = plt.imshow(W1[i, :].reshape([28,28]).T, cmap='gray')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
